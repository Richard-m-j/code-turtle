# .github/workflows/reusable-review.yml

name: Reusable AI Code Review

on:
  workflow_call:
    inputs:
      pr_number:
        description: 'The number of the pull request'
        required: true
        type: number
      code_diff:
        description: 'The code diff string to be reviewed'
        required: true
        type: string

# Reusable workflow should also declare the permissions it needs
permissions:
  pull-requests: write
  contents: read

jobs:
  code-review:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    container: yusiwen/llama.cpp:latest
    env:
      MODEL_URL: 'https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf'

    steps:
      - name: 1. Generate Model URL Hash
        id: model_hash
        run: echo "hash=$(echo -n '${{ env.MODEL_URL }}' | sha256sum | cut -d' ' -f1)" >> $GITHUB_OUTPUT

      - name: 2. Cache AI Model
        id: cache-model
        uses: actions/cache@v4
        with:
          path: ./model.gguf
          key: ${{ runner.os }}-gguf-model-${{ steps.model_hash.outputs.hash }}

      - name: 3. Download AI Model if not cached
        if: steps.cache-model.outputs.cache-hit != 'true'
        run: |
          set -euo pipefail
          echo "Model cache miss â€” downloading model to ./model.gguf"
          mkdir -p "$(pwd)"
          curl -fL -o ./model.gguf "${{ env.MODEL_URL }}"
          echo "Download complete: $(stat -c '%s' ./model.gguf 2>/dev/null || true) bytes"

      - name: 4. Construct Prompt (write diff safely to a file)
        env:
          CODE_DIFF: ${{ inputs.code_diff }}
        run: |
          set -euo pipefail
          PROMPT_FILE="prompt.txt"
          echo "Writing prompt to $PROMPT_FILE"

          cat > "$PROMPT_FILE" <<'SYS'
          You are an expert code reviewer AI. Your task is to analyze the following code diff and provide concise, constructive feedback. Focus on potential bugs, logical errors, style violations, and areas for improved clarity or performance. Do not comment on trivial changes. Frame your feedback as helpful suggestions. Output your review in Markdown format. SYS

          echo "" >> "$PROMPT_FILE"
          echo "Here is the code diff to review:" >> "$PROMPT_FILE"
          echo '```diff' >> "$PROMPT_FILE"

          # Append the diff from the input (preserves newlines)
          printf '%s\n' "$CODE_DIFF" >> "$PROMPT_FILE"

          echo '```' >> "$PROMPT_FILE"

          echo "Prompt written (first 1KB):"
          head -c 1024 "$PROMPT_FILE" || true

      - name: 5. Preflight checks for llama binary & run LLM Inference
        env:
          LD_LIBRARY_PATH: /llama.cpp
        run: |
          set -euo pipefail
          echo "Checking for llama binary and model file"

          # Basic checks
          if [ ! -x /llama.cpp/llama-cli ]; then
            echo "WARNING: /llama.cpp/llama-cli not executable or not present. Listing /llama.cpp for debug:"
            ls -la /llama.cpp || true
            # try one more common path
            if [ -x ./llama-cli ]; then
              CLI_PATH="./llama-cli"
            else
              echo "ERROR: llama-cli not found. Exiting."
              exit 1
            fi
          else
            CLI_PATH="/llama.cpp/llama-cli"
          fi

          # Check that model file exists (if using cached file)
          if [ ! -f ./model.gguf ]; then
            echo "Warning: ./model.gguf not present in workspace. If you expected it to be cached/downloaded, check prior steps."
          else
            echo "Model file size: $(stat -c '%s' ./model.gguf || true) bytes"
          fi

          echo "Starting LLM inference (if present) â€” output will be written to review_output.txt"
          # Keep predictions modest in case runner memory is limited. Adjust flags as needed.
          "$CLI_PATH" -m ./model.gguf -f prompt.txt --ctx-size 4096 --n-predict 512 --temp 0.2 > review_output.txt || true
          echo "LLM inference step finished (exit code ignored to allow posting whatever we have)."

          echo "Preview of review output (first 2KB):"
          head -c 2048 review_output.txt || true

      - name: 6. Format and Post Comment (use actions/github-script â€” no apt-get)
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const path = 'review_output.txt';
            let body = '';
            try {
              body = fs.readFileSync(path, 'utf8');
            } catch (err) {
              body = '*No model output was produced or the file could not be read.*\n\n' + String(err);
            }

            const comment = [
              "### ðŸ¤– AI Code Review",
              "",
              "Here are some suggestions based on the latest changes. Please treat these as pointers for consideration, not mandates.",
              "",
              "---",
              "",
              body
            ].join("\n");

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: parseInt(process.env.INPUT_PR_NUMBER || process.env.PR_NUMBER || '${{ inputs.pr_number }}'),
              body: comment
            });
        env:
          PR_NUMBER: ${{ inputs.pr_number }}

      - name: 7. Upload full review output as artifact (optional, helps with large content)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ai-review-output-${{ inputs.pr_number }}
          path: |
            review_output.txt
            prompt.txt
            /tmp/full_diff.txt
