name: Run Llama.cpp Inference using Public Docker Image

on:
  push:
    branches:
      - '**'
  workflow_dispatch:
    inputs:
      prompt:
        description: 'The prompt to send to the model'
        required: true
        default: 'What is the speed of light?'
      model_url:
        description: 'URL of the GGUF model to download'
        required: true
        default: 'https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf'

jobs:
  run-inference:
    runs-on: ubuntu-latest
    
    # Run all steps directly inside the pre-built llama.cpp container
    container: yusiwen/llama.cpp:latest

    steps:
      # Step 1: Cache the downloaded AI model to speed up subsequent runs
      - name: Cache AI Model
        id: cache-model
        uses: actions/cache@v4
        with:
          path: ./model.gguf
          # Create a unique key for the cache based on the model URL
          key: ${{ runner.os }}-gguf-model-${{ hashFiles(inputs.model_url) }}

      # Step 2: Download the AI model only if it's not found in the cache
      - name: Download AI Model if not cached
        if: steps.cache-model.outputs.cache-hit != 'true'
        run: curl -L -o ./model.gguf "${{ inputs.model_url }}"
              
      # Step 3: Run the inference using the pre-compiled binary in the container
      - name: Run Inference
        run: |
          /usr/src/app/main \
            -m ./model.gguf \
            -p "${{ inputs.prompt }}" \
            -n 128 \
            -ngl 0