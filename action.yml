# action.yml

name: 'Code-Turtle AI Code Review'
description: 'An AI-powered code reviewer that analyzes pull requests and provides feedback.'
author: 'Richard Joseph'

inputs:
  pr_number:
    description: 'The number of the pull request to review.'
    required: true
  model_url:
    description: 'URL of the GGUF model to download for the review.'
    required: false
    default: 'https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf'
  github_token:
    description: 'The GitHub token for posting comments.'
    required: true

  repository:
    description: 'The owner and repository name (e.g., octocat/Hello-World).'
    required: true

runs:
  using: 'docker'
  image: 'ghcr.io/richard-m-j/llama.cpp:light'
  env:
    MODEL_URL: ${{ inputs.model_url }}
    GITHUB_TOKEN: ${{ inputs.github_token }}
    PR_NUMBER: ${{ inputs.pr_number }}
    GH_REPO: ${{ inputs.repository }}
  entrypoint: '/bin/bash'
  args:
    - -c
    - |
      set -e # Exit immediately if a command exits with a non-zero status.

      # Step 1: Download Diff Artifact
      # In a marketplace action, you would typically get the diff directly,
      # but to keep the logic similar, we'll assume the diff is created
      # in a previous step and is available in the workspace.
      # The calling workflow will need to handle the diff generation.
      
      # Step 2: Generate Model URL Hash
      MODEL_HASH=$(echo -n "$MODEL_URL" | sha256sum | cut -d' ' -f1)

      # Step 3: Cache and Download AI Model (simplified for action)
      # Caching is handled by the actions/cache action in the calling workflow
      if [ ! -f ./model.gguf ]; then
        echo "Downloading AI Model..."
        curl -L -o ./model.gguf "$MODEL_URL"
      fi

      # Step 4: Construct Prompt
      PROMPT_FILE="prompt.txt"
      SYSTEM_PROMPT="You are an expert code reviewer AI..." # Truncated for brevity
      CODE_DIFF=$(cat code.diff)
      echo "$SYSTEM_PROMPT" > $PROMPT_FILE
      echo "" >> $PROMPT_FILE
      echo "Here is the code diff to review:" >> $PROMPT_FILE
      echo '```diff' >> $PROMPT_FILE
      echo "$CODE_DIFF" >> $PROMPT_FILE
      echo '```' >> $PROMPT_FILE
      echo "" >> $PROMPT_FILE
      echo "Please provide your review now." >> $PROMPT_FILE

      # Step 5: Run LLM Inference
      /home/appuser/llama-cli \
        -m ./model.gguf \
        -f prompt.txt \
        --ctx-size 4096 \
        --n-predict 1024 \
        --temp 0.2 \
        > review_output.txt

      # Step 6: Format and Post Comment
      echo "### ðŸ¤– AI Code Review" > final_comment.md
      echo "" >> final_comment.md
      echo "Here are some suggestions based on the latest changes..." >> final_comment.md
      echo "" >> final_comment.md
      echo "---" >> final_comment.md
      echo "" >> final_comment.md
      cat review_output.txt >> final_comment.md
      
      gh pr comment $PR_NUMBER --repo $GH_REPO --body-file final_comment.md