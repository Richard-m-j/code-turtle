name: Run Llama.cpp Inference using Public Docker Image

on:
  push:
    branches:
      - '**'
  workflow_dispatch:
    inputs:
      prompt:
        description: 'The prompt to send to the model'
        required: true
        default: 'What is the speed of light?'
      model_url:
        description: 'URL of the GGUF model to download'
        required: true
        default: 'https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf'

jobs:
  run-inference:
    runs-on: ubuntu-latest
    
    container: yusiwen/llama.cpp:latest

    env:
      MODEL_URL: ${{ github.event_name == 'workflow_dispatch' && inputs.model_url || 'https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf' }}

    steps:
      - name: Generate Model URL Hash
        id: model_hash
        run: echo "hash=$(echo -n '${{ env.MODEL_URL }}' | sha256sum | cut -d' ' -f1)" >> $GITHUB_OUTPUT

      - name: Cache AI Model
        id: cache-model
        uses: actions/cache@v4
        with:
          path: ./model.gguf
          key: ${{ runner.os }}-gguf-model-${{ steps.model_hash.outputs.hash }}

      - name: Download AI Model if not cached
        if: steps.cache-model.outputs.cache-hit != 'true'
        run: curl -L -o ./model.gguf "${{ env.MODEL_URL }}"
              
      - name: Run Inference
        run: |
          /llama-cli \
            -m ./model.gguf \
            -p "${{ inputs.prompt }}" \
            -n 128 \
            -ngl 0